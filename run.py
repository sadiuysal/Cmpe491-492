# -*- coding: utf-8 -*-
"""beginner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb

##### Copyright 2019 The TensorFlow Authors.
"""

#@title Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import keras as keras
import matplotlib.pyplot as plt
from tensorflow.keras import layers
import objective as obj_lib
import data_util
import tensorflow as tf
import numpy as np
import sys
import model as model_class
from tensorflow import keras

tf.config.run_functions_eagerly(True)

x_train, x_test = model_class.x_train , model_class.x_test

batch_size=64
indicies=np.array([i for i in range(batch_size)]).reshape((batch_size, 1))


IMG_SIZE=tf.shape(x_train)[1]
#resize_and_scale with layers
resize_and_rescale = model_class.resize_and_rescale
#data augmentation layers
data_augmentation = model_class.data_augmentation

inputs=keras.Input(shape=(32, 32, 3))
"""Build the `tf.keras.Sequential` model by stacking layers. Choose an optimizer and loss function for training:"""
model = model_class.CustomModel(inputs,model_class.model_layers(inputs))



def prepare(ds=x_train, shuffle=False, augment=True,batch_size = 32):
  AUTOTUNE = tf.data.experimental.AUTOTUNE
  # Resize and rescale all datasets
  ds = ds.map(lambda x, y: (augment(x), y), 
              num_parallel_calls=AUTOTUNE)
  if shuffle:
    ds = ds.shuffle(1000)

  # Batch all datasets
  ds = ds.batch(batch_size)

  # Use data augmentation only on the training set
  if augment:
    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), 
                num_parallel_calls=AUTOTUNE)

  # Use buffered prefecting on all datasets
  return ds.prefetch(buffer_size=AUTOTUNE)

def display_aug_images():
  image=x_train[0]
  # Add the image to a batch
  image = tf.expand_dims(image, 0)
  plt.figure(figsize=(10, 10))
  for i in range(9):
    augmented_image = data_augmentation(image)
    print(augmented_image)
    ax = plt.subplot(3, 3, i + 1)
    plt.show(augmented_image[0],block=True)
    plt.axis("off")


loss_fn = obj_lib.contrastive_Loss
def train():
  model.compile(optimizer='adam') # ['accuracy'])
  """ The `Model.fit` method adjusts the model parameters to minimize the loss: """
  model.fit(x_train, epochs=5 , batch_size=batch_size)
  """The `Model.evaluate` method checks the models performance, usually on a "[Validation-set](https://developers.google.com/machine-learning/glossary#validation-set)" or "[Test-set](https://developers.google.com/machine-learning/glossary#test-set)"."""

  #model.evaluate(x_test,  y_test, verbose=2)

  """The image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).

  If you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:
  """
  """
  probability_model = tf.keras.Sequential([
    model,
    tf.keras.layers.Softmax()
  ])

  probability_model(x_test[:5])
  """
input=tf.slice(x_train,[0,0,0,0],[ batch_size ,-1 ,-1 ,-1 ])
input_2N=tf.concat([input, input], 0)

#print(loss_fn(model(input_2N)))
train()



